CSE 158R
Harshita Saha
Assignment 1
----------------------------------------------------------------------------------------------------------------------------------
                                                    Task 1 [Read prediction]:
----------------------------------------------------------------------------------------------------------------------------------

First I began by loading in the complete dataset as a list of all ratings from the "train_Interactions.csv.gz" file.

I also created ratingsTrain and ratingsValid lists, and used the ratingsValid list to add negative tests to the validation set (users and book pairs, with a book being one that the specific paired user has not read).

I iterated over all the data in the "train_Interactions.csv.gz" file to generate a list of the most read books and how many times they were read (mostPopular), and a set of these book IDs (return1), for the top 65% of read books.   

I defined a Jaccard similarity function (Jaccard), and used that to create two functions (jacc_user and jacc_item), which given a user and book pair, return an array of jaccard similarities using data from the Training set. jacc_user returns the similarities between the users who read the passed book, and users who read other books read by the passed user. jacc_item returns the similarities between the books read by the passed user, and books read by other users who read the passed book. 

I also generated a threshold for jaccard similarity by finding the output of jacc_user for all user and book pairs in ratingsValid, and selected the threshold as being the 85th percentile of jacc_user outputs (threshold_u = 0.0035087719298245615). Using the 85th percentile was a random choice. 

I then used the mostPopular list to create a dictionary, with book IDs as indices and the number of times that book was read as the values. I then used this dictionary to create a function called popularity, that returns the number of times the book that is passed was read. 

I then defined a feature function, that for each passed user and book pairs, returns an array of the form [1, popularity, max(jacc_u), max(jacc_i]. I then generated a list, X1, containing feature outputs for all datapoints in ratingsValid, and a list y1, with the true read values for the same datapoints. 

I used a model (mod1), linear_model.LogisticRegression(C=1, class_weight = 'balanced'), and used it to fit X1 and y1. 

I then used my jaccard threshold to create a new threshold (threshold_u*0.65, which was equal to 0.002280701754385965). This was a random choice generated by the fact that I was focusing on the 65% of most read books, and it worked the best of all other options I had tried. 

I also then found the last book that was added to return1, and calculated the number of times it was read using the dictionary of book IDs and the number of times they were read (44). I did this to account for that fact that there could be books not added to the return1 set, that were read the same number of times as the last book in the set, as the set of 65% top read books was influenced by the order in which the books were sorted in mostPopular.

I then created my predictor function. If there were no books read by the user in the training set, I said the book was read if the book was in return1 (top 65% of read books), or if it was read at least 44 times. Else, I used mod1 to predict whether or not the book was read. 

In the case that there were books read by the user in the training set, if the mean of the jacc_user output of the user and book pair exceeded threshold_u*0.65 or if the passed book was in return1, I predicted read. I also predicted read if the book was read at least 44 times. Finally, if all these conditions were not fulfilled, I used mod1 to predict whether or not the user read that passed book.

----------------------------------------------------------------------------------------------------------------------------------
                                                    Task 2 [Category prediction]:
----------------------------------------------------------------------------------------------------------------------------------

First I began by importing stopwords from nltk.corpus, and used that to create a list of stopwords. I also loaded in the data from the file "train_Category.json.gz" into a list. 

I then generated a dictionary of words (wordCount), that contained keys for all words across all reviews in the data, and whose keys were the counts of occurences of the words. Counts were only added to for non-stopwords, such that stopword keys would have values of 0. 

Using the wordCounts dictionary, I created a list of tuples (counts), sorted by the number of times they occurred across all reviews in descending order. I used counts to create a list of the top 7000 most common words (words), a set of these words (wordSet), and a dictionary of the words and a ranking of how common they are (wordId).

I then defined a feature function and generated a list X2, of features for data points in data, and list y2, of actual genreID values for data points in data. I then split X2 and y2 into training and validation sets, and used the model mod2 = linear_model.LogisticRegression(C=1), fitting mod2 on Xtrain and ytrain. 

I then loaded in the data from "test_Category.json.gz", generated a list of features Xtest, for datapoints in the test data, and used mod2 to generate an array of predictions for each datapoint in the test data. 


